\section{Introduction}
\label{intro}

The Network File System (NFS) allows clients to access data seamlessly over the network. This is accomplished through the same system calls that allow the access of files on the local disk. In the case of NFS these system calls trigger the client stub which sends an RPC across the network to an NFS server. These traditional reads/writes are synchronous by default, which blocks the execution of the application until each of the requested I/O operation is completed. Conversely, asynchronous I/O enables the applications to continue processing while several I/O operations are running in the background. This feature allows the applications to overlap their compute and I/O processing to improve throughput on a per process basis. In Linux, the POSIX asynchronous I/O \cite{aio} provides us with system calls to initiate one or more I/O operations asynchronously. Typically an application using an asynchronous I/O interface submits batch I/O and waits for the completion of all the requests in the batch. Similarly, batch I/O operations can also be performed on the files mounted on NFS. In the case of NFS, the client NFS stub triggers each of these requests synchronously and waits for the response before triggering the next request. The time taken for the batch I/O request can be greatly reduced if these requests can be triggered concurrently instead of client waiting for each of the requests to be completed before triggering the next. One reason for this sequential request forwarding is, until NFSv4.0, there is no way by which the server can connect to the client unless the connection is initiated by client. NFSv4.0 had provided a feature of callbacks by which servers can contact the client, there by the server in the callback request will act as a client and the client will act as server. This callback feature can be used for making the NFS asynchronous I/O truly asynchronous. So, the clients can trigger the requests concurrently and then wait for the response. The server fetches the data from disk,  uses the callback mechanism to send the data to the client. On receiving the callbacks for all the requests, the client NFS stub signals the completion  to the  application. This would greatly enhance the throughput of the application as the wait time for the client has reduced. In summary the existing implementations of asynchronous I/O operations in NFS are not fully exploiting the benefits of asynchronous nature.

For implementing the asynchronous I/O, we have used NFS-Ganesha \cite{ganesha} as the NFS server and Pynfs \cite{pynfs} as the NFS client. We have used the latest stable version of NFS that is v4.1 for both client and server. NFS-Ganesha is an user level implementation of the NFS server in \textit{C} language. We chose NFS-Ganesha because it is easy to enhance NFS-Ganesha when compared to the kernel NFS source code. NFS-Ganesha is also actively used by a wider audience and also supported by major companies like IBM, Panasas and Redhat. Hence it supports all the latest features of the NFSv4.1 like sessions, callbacks. Pynfs is an user level implementation of NFS client and server in \textit{python}, it is used as a test suite for checking the correctness of NFS protocol.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% For Emacs:
% Local variables:
% fill-column: 70
% End:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% For Vim:
% vim:textwidth=70
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LocalWords:
