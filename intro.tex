\section{Introduction}
\label{sec:intro}

\begin{figure*}
\centering
\includegraphics[scale=0.9]{figures/ReadSequence.eps}
\caption{NFS Read Sequence Diagram}
\label{fig:NFSRead}
\end{figure*}
The Network File System (NFS) allows clients to access data seamlessly over the network. This is accomplished through the same system calls that allow the access of files on the local disk. In the case of NFS these system calls trigger the client stub which sends an RPC across the network to an NFS server. These traditional reads/writes are synchronous by default, which blocks the execution of the application until each of the requested I/O operation is completed. Conversely, asynchronous I/O enables the applications to continue processing while several I/O operations are running in the background. This feature allows the applications to overlap their compute and I/O processing to improve throughput on a per process basis. In Linux, the POSIX asynchronous I/O \cite{aio} provides us with system calls to initiate one or more I/O operations asynchronously. Typically an application using an asynchronous I/O interface submits batch I/O and waits for the completion of all the requests in the batch. Similarly, batch I/O operations can also be performed on the files mounted on NFS. In the case of NFS, the NFS client stub triggers each of these requests synchronously and waits for the response before triggering the next request. The time taken for the batch I/O requests can be greatly reduced if these requests can be triggered concurrently instead of client waiting for each of the requests to be completed before triggering the next. NFSv4.0 had provided a feature of callbacks by which servers can contact the client, there by the server in the callback request will act as a client and the client will act as server. This callback feature can be used for making  asynchronous I/O on NFS truly asynchronous. So, the clients can trigger the requests concurrently and then wait for the response. The server fetches the data from the disk, and uses the callback mechanism to send the data to the client. On receiving the callbacks for all the requests, the client NFS stub signals the completion to the  application. This would greatly enhance the throughput of the application. In summary the current implementation of asynchronous I/O operations is not synchronous at NFS Level.

For implementing the asynchronous I/O, we have used NFS-Ganesha \cite{ganesha} as the NFS server and Pynfs \cite{pynfs} as the NFS client. We have used the latest stable version of NFS that is v4.1 for both client and server. NFS-Ganesha is an user level implementation of the NFS server written in \textit{C} language. We chose NFS-Ganesha because it is easy to enhance NFS-Ganesha when compared to the kernel NFS source code. NFS-Ganesha is also actively used by a wider audience and also supported by major companies like IBM, Panasas and Redhat \cite{NFSGanesha}. Hence it supports all the latest features of the NFSv4.1 like sessions, callbacks \cite{NFSv41rfc}. Pynfs is an user level implementation of NFS client and server written in \textit{python}, it is used as a test suite for checking the correctness of NFS protocol.

\begin{figure*}
\centering
\includegraphics[scale=0.75]{figures/AsyncSequence.eps}
\caption{NFS Async Read Sequence Diagram}
\label{fig:NFSAsyncRead}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% For Emacs:
% Local variables:
% fill-column: 70
% End:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% For Vim:
% vim:textwidth=70
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LocalWords:
