\section{Evaluation}
\label{sec:Evaluation}

\begin{figure*}
\centering
\includegraphics[scale=0.7]{figures/Slotreuse.eps}
\caption{NFS asynchronous read slot reuse diagram}
\label{fig:NFSSlotreuse}
\end{figure*}


	We have ran our experiments by running the NFS-Ganesha server on Amazon EC2 and Pynfs client on the local machine to replicate a real world application.  We have evaluated asynchronous read operation based on two metrics, one is the \textit{completion time} and the other is the \textit{throughput}. \textit{Completion time} is the total time taken for the completion of \textsc{NFS read} operation and \textsc{NFS asynchronous read} operation. \textit{Throughput} is a comparision between the time taken for the initial \textsc{nfs4\_ok} to reach the client incase of asynchronous read and the total time taken by the \textsc{NFS read}. We have compared our results with the normal read request. The data size we read in performing normal read and asynchronous read are 512 bytes, 1MB, 2MB and 10MB. 


Figure~\ref{fig:NFSCompletionTimes} shows the  difference in the completion times between normal read and asynchronous read. We observed that the time taken for normal and asynchronous read is almost similar. This suggests that the overhead due to an additional callback incase of asynchronous read is not significant. Thus Asynchronous READ operation can be used in all the scenarios where client does not require data to continue its operations as there is no overhead. Also the growth in the file size had a similar effect on both \textsc{NFS read} and \textsc{NFS asynchronous read}.


Figure~\ref{fig:NFSThroughput} compares the  time taken for the intial \textsc{nfs4\_ok} to reach the client incase of NFS asynchronous read and the total time taken by the \textsc{NFS read}.
Once the client recieves a \textsc{nfs4\_ok}  incase of asynchronous read from the server, it is free to perform other actvities. Thus the time difference can be considered as the throughput gain for the client. We have analysed the throughput gain incase on single request and 2 consequitve requests on a file size of 1MB. The offsets in the file were choosen carefully in a way that neither the dcache nor \textsc{read ahead} will effect the final outcome. In case of a single request it took 78 milliseconds for the 
\textsc{NFS read} to get completed. And it took around 21 milliseconds incase of asynchronous read for the initial \textsc{nfs4\_ok}  to reach the client. 



 Figure~\ref{fig:InterstingObservation} shows an intersting observation. When we have made two consecutive reads  using \textsc{NFS read} on a file of size 1GB. While the first read has a offset of 1MB, the second read has a offset of 512 KB. The size of the data read is 512KB in both the cases. The file offsets are choosen this way to prevent \textsc{read ahead} caching. While it has taken 88 milliseconds  in \textsc{NFS read} for both the requests to get completed, it has taken only 68 milliseconds for \textsc{NFS asynchronous read}. This can attributed to the scenario depicted  in   Figure~\ref{fig:performancesequence}. Incase of \textsc{NFS read}, second read request is made only after the client recieves the reply to the first request. Thus there is no chance for both of them to be scheduled together to the disk. But in the case of asynchronous read, Since the second request is triggered immediately after receiving the  \textsc{nfs4\_ok}  to the first one, the two requests can be scheduled together to the disk. Thus there is a time gain since the seek head does not have to do a full rotation to reach the second position as both the experiments are scheduled together.    






